<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://chefssalad.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://chefssalad.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-22T00:41:49+00:00</updated><id>https://chefssalad.github.io/feed.xml</id><title type="html">blank</title><subtitle>Cheng Chen&apos;s personal website </subtitle><entry><title type="html">Visual Perception-Enhanced Segmentation</title><link href="https://chefssalad.github.io/2025/10/13/enhanced-segmentation-md.html" rel="alternate" type="text/html" title="Visual Perception-Enhanced Segmentation"/><published>2025-10-13T00:00:00+00:00</published><updated>2025-10-13T00:00:00+00:00</updated><id>https://chefssalad.github.io/2025/10/13/enhanced-segmentation-md</id><content type="html" xml:base="https://chefssalad.github.io/2025/10/13/enhanced-segmentation-md.html"><![CDATA[<h4 id="-competition--recognition">üèÜ <strong>Competition &amp; Recognition</strong></h4> <p>This project achieved <strong>1st place</strong> in the AI medical image segmentation track co-organized by <strong>Shanghai AI Laboratory</strong> and <strong>Ruijin Hospital, Shanghai Jiao Tong University School of Medicine</strong>.<br/> It was subsequently invited for presentation at the <strong>2023 Healthy China Sinan Summit</strong> and the <strong>Frontiers of Digital Medicine Forum</strong>.</p> <hr/> <h4 id="-project-overview">üß† <strong>Project Overview</strong></h4> <p>This project introduces a novel <strong>Visual Perception-Enhanced Segmentation (VES)</strong> framework designed for lightweight and accurate medical image segmentation.<br/> It focuses on preprocessing, slice-wise attention, and collaborative dual-branch perception to enhance robustness and precision.</p> <hr/> <h4 id="-contributions-overview">üîß <strong>Contributions Overview</strong></h4> <h5 id="1--effective-data-preprocessing">1. üßº <strong>Effective Data Preprocessing</strong></h5> <p>This method analyzes the Hounsfield Unit (HU) distribution from a colorectal tumor CT dataset and designs a two-step preprocessing pipeline:</p> <ul> <li><strong>HU-based intensity truncation</strong></li> <li><strong>Noise cropping using object detection</strong></li> </ul> <p align="center"> <img src="/assets/img/VES/preprocessing.png" alt="Data Preprocessing Pipeline" width="60%"/> </p> <p align="center"><em>Figure 1. Data preprocessing pipeline for HU clipping and noise removal.</em></p> <hr/> <h5 id="2--lightweight-25d-slice-wise-attention">2. üß† <strong>Lightweight 2.5D Slice-Wise Attention</strong></h5> <p>We propose a 2.5D data structure where adjacent slices are stacked as channels.<br/> A slice-wise attention module dynamically reweights each channel to focus on informative regions.</p> <p align="center"> <img src="/assets/img/VES/sliceattention.png" alt="Slice Attention Mechanism" width="60%"/> </p> <p align="center"><em>Figure 2. Slice-wise attention mechanism highlights important slices.</em></p> <p align="center"> <img src="/assets/img/VES/SAH.png" alt="Channel Attention Effects" width="60%"/> </p> <p align="center"><em>Figure 3. Visualization of slice-level attention weights.</em></p> <hr/> <h5 id="3--enhanced-visual-perception-module">3. üß© <strong>Enhanced Visual Perception Module</strong></h5> <p>We design a <strong>dual-branch co-attention mechanism</strong> to fuse global context and multi-scale local texture features, enabling strong segmentation under complex conditions.</p> <p align="center"> <img src="/assets/img/VES/MBCA.png" alt="Dual-branch Co-attention" width="60%"/> </p> <p align="center"><em>Figure 4. Visual perception module combining long-range and local texture cues.</em></p> <hr/> <h4 id="-mathematical-notations">üßÆ <strong>Mathematical Notations</strong></h4> <h5 id="hounsfield-unit-hu">Hounsfield Unit (HU):</h5> \[HU = 1000 \times \frac{\mu - \mu_{water}}{\mu_{water}}\] <p>Where:</p> <ul> <li>$\mu$: Attenuation coefficient of tissue</li> <li>$\mu_{water}$: Attenuation coefficient of water</li> </ul> <h5 id="normalization">Normalization:</h5> \[x_{norm} = \frac{clip(x, HU_{min}, HU_{max}) - HU_{min}}{HU_{max} - HU_{min}}\] <p>Where $HU_{min}$ and $HU_{max}$ are clipped based on the 0.05% percentile tails.</p> <hr/> <h4 id="-experimental-results">üìä <strong>Experimental Results</strong></h4> <h5 id="-visual-segmentation-results">‚úÖ <strong>Visual Segmentation Results</strong></h5> <p>The following visualizations demonstrate segmentation overlays and prediction reliability:</p> <p align="center"> <img src="/assets/img/VES/results.png" alt="Segmentation Overlay" width="75%"/> </p> <p align="center"><em>Figure 5. Prediction overlay: green (FN), red (FP), yellow (TP).</em></p> <hr/> <h5 id="-quantitative-comparisons">üìà <strong>Quantitative Comparisons</strong></h5> <p align="center"> <img src="/assets/img/VES/radar.png" alt="Radar Plot" width="75%"/> </p> <p align="center"><em>Figure 6. Radar plot comparing evaluation metrics across methods.</em></p> <p align="center"> <img src="/assets/img/VES/box.png" alt="Box Plot" width="75%"/> </p> <p align="center"><em>Figure 7. Dice coefficient distribution across test samples.</em></p>]]></content><author><name></name></author><category term="Medical Imaging"/><category term="Segmentation"/><category term="Deep Learning"/><category term="AI Competition"/><summary type="html"><![CDATA[A lightweight and robust visual perception-enhanced segmentation framework achieving 1st place in a medical AI challenge.]]></summary></entry></feed>